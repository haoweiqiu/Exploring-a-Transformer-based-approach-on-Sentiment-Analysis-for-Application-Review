{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Christopher Chong, StudentID: 260976714\n",
        "# this code is for running experiments on CNN and LSTM\n",
        "\n",
        "# using keras for model implementation\n",
        "\n",
        "# load in datasets\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"app_reviews\")\n",
        "\n",
        "\n",
        "# convert to pandas dataframe\n",
        "import pandas as pd\n",
        "df = dataset['train'].to_pandas()\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "dataset = df[[\"review\", \"star\"]]\n",
        "\n"
      ],
      "metadata": {
        "id": "175NCjDlgl6S",
        "outputId": "77a8a723-fa94-49b6-a8cd-3c24201b8b13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkot6PDif7OJ",
        "outputId": "68f758ef-0043-4610-d405-0f3e3dead167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing for the data\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def review_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    lemmatize = WordNetLemmatizer()\n",
        "    text = [lemmatize.lemmatize(word) for word in text]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    return text\n",
        "\n",
        "dataset['review'] = dataset['review'].apply(lambda x: review_preprocess(x))\n",
        "dataset['review'] = dataset['review'].astype(str)\n",
        "dataset"
      ],
      "metadata": {
        "id": "5DVr3re-wFCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "bf6492c4-3fcd-48d4-96ab-a0ec39fd03c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   review  star\n",
              "34430   ['love', '!', '!', '!', '!', 'far', 'best', 'r...     5\n",
              "170339                                     ['liked', 'üëç']     4\n",
              "116866  ['nice', 'ui', 'doe', \"n't\", 'work', 'utorrent...     3\n",
              "76784                            ['u3fjahat', 'u3fjahat']     5\n",
              "246010                                     ['nice', 'ok']     1\n",
              "...                                                   ...   ...\n",
              "141200  ['?', '?', 'app', 'ha', 'updated', 'long', 'ti...     3\n",
              "10708                      ['best', 'best', 'app', 'coc']     5\n",
              "84491   ['publish', 'publish', 'thing', 'like', 'text'...     2\n",
              "148526                                  ['ÿπÿßŸÑ€åŸá', 'good']     5\n",
              "26522   ['excellent', 'tool', 'maximum', 'battery', 'l...     5\n",
              "\n",
              "[57613 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68835c12-9484-4f75-8a0b-ea417beed39c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>star</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34430</th>\n",
              "      <td>['love', '!', '!', '!', '!', 'far', 'best', 'r...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170339</th>\n",
              "      <td>['liked', 'üëç']</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116866</th>\n",
              "      <td>['nice', 'ui', 'doe', \"n't\", 'work', 'utorrent...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76784</th>\n",
              "      <td>['u3fjahat', 'u3fjahat']</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246010</th>\n",
              "      <td>['nice', 'ok']</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141200</th>\n",
              "      <td>['?', '?', 'app', 'ha', 'updated', 'long', 'ti...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10708</th>\n",
              "      <td>['best', 'best', 'app', 'coc']</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84491</th>\n",
              "      <td>['publish', 'publish', 'thing', 'like', 'text'...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148526</th>\n",
              "      <td>['ÿπÿßŸÑ€åŸá', 'good']</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26522</th>\n",
              "      <td>['excellent', 'tool', 'maximum', 'battery', 'l...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57613 rows √ó 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68835c12-9484-4f75-8a0b-ea417beed39c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68835c12-9484-4f75-8a0b-ea417beed39c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68835c12-9484-4f75-8a0b-ea417beed39c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bff7091d-8bc4-4cda-a83b-3d9bfc795e92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bff7091d-8bc4-4cda-a83b-3d9bfc795e92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bff7091d-8bc4-4cda-a83b-3d9bfc795e92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ed2597f9-272b-4d8d-953e-e6ee36359cc1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_sample')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ed2597f9-272b-4d8d-953e-e6ee36359cc1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_sample');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = dataset['review'].values.tolist()\n",
        "\n",
        "labels = np.array(dataset['star'])\n",
        "\n",
        "# turn labels into data neural network ccan use\n",
        "labels = tf.keras.utils.to_categorical(labels, 6, dtype=\"float32\")\n"
      ],
      "metadata": {
        "id": "svB4SNZPxvNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from keras import layers\n"
      ],
      "metadata": {
        "id": "MDNnEnapVGmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "# converting text data into data usable by the neural network using official keras tensorflow documentation:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text_data) #encodes the data used to train from texts into usable sequences\n",
        "sentences = pad_sequences(sequences) # ensures all sequences in the list gets the same length"
      ],
      "metadata": {
        "id": "oK35XqoKwied"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data, # DOUBLE CHECKED\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.25, random_state=0)\n"
      ],
      "metadata": {
        "id": "iw7Ujxbiw9d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 1 - CNN EPOCH = 10\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lfn-e__Ks6Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUpArQC_3axP",
        "outputId": "e8e31778-f609-4360-dfd2-4c808a677e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.1206 - acc: 0.6191 - val_loss: 1.0662 - val_acc: 0.6331\n",
            "Epoch 2/10\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0520 - acc: 0.6394 - val_loss: 1.0458 - val_acc: 0.6415\n",
            "Epoch 3/10\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0407 - acc: 0.6434 - val_loss: 1.0414 - val_acc: 0.6420\n",
            "Epoch 4/10\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0341 - acc: 0.6447 - val_loss: 1.0367 - val_acc: 0.6437\n",
            "Epoch 5/10\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0301 - acc: 0.6454 - val_loss: 1.0550 - val_acc: 0.6353\n",
            "Epoch 6/10\n",
            "3376/3376 [==============================] - 64s 19ms/step - loss: 1.0275 - acc: 0.6461 - val_loss: 1.0313 - val_acc: 0.6446\n",
            "Epoch 7/10\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.0255 - acc: 0.6467 - val_loss: 1.0309 - val_acc: 0.6432\n",
            "Epoch 8/10\n",
            "3376/3376 [==============================] - 66s 20ms/step - loss: 1.0238 - acc: 0.6466 - val_loss: 1.0277 - val_acc: 0.6442\n",
            "Epoch 9/10\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0226 - acc: 0.6464 - val_loss: 1.0285 - val_acc: 0.6444\n",
            "Epoch 10/10\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0212 - acc: 0.6468 - val_loss: 1.0277 - val_acc: 0.6441\n",
            "2251/2251 - 6s - loss: 1.0277 - acc: 0.6441 - 6s/epoch - 3ms/step\n",
            "Training time: 623.0022926330566s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  10\n",
            "Batch Size:  64\n",
            "Model loss:  1.027712106704712\n",
            "Model accuracy:  0.6440562605857849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 2 - CNN EPOCH = 20\n"
      ],
      "metadata": {
        "id": "BO3GRP2atAUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHux-aiwcat6",
        "outputId": "413105b0-c09f-4509-ef75-08ae19812b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.1071 - acc: 0.6236 - val_loss: 1.0723 - val_acc: 0.6307\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0519 - acc: 0.6383 - val_loss: 1.0460 - val_acc: 0.6382\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0384 - acc: 0.6419 - val_loss: 1.0377 - val_acc: 0.6398\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 67s 20ms/step - loss: 1.0305 - acc: 0.6443 - val_loss: 1.0326 - val_acc: 0.6431\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0270 - acc: 0.6454 - val_loss: 1.0289 - val_acc: 0.6428\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0245 - acc: 0.6465 - val_loss: 1.0279 - val_acc: 0.6443\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0228 - acc: 0.6467 - val_loss: 1.0253 - val_acc: 0.6444\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0208 - acc: 0.6477 - val_loss: 1.0243 - val_acc: 0.6442\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0198 - acc: 0.6480 - val_loss: 1.0316 - val_acc: 0.6435\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0187 - acc: 0.6480 - val_loss: 1.0249 - val_acc: 0.6442\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0172 - acc: 0.6490 - val_loss: 1.0310 - val_acc: 0.6440\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0156 - acc: 0.6494 - val_loss: 1.0221 - val_acc: 0.6464\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0144 - acc: 0.6499 - val_loss: 1.0238 - val_acc: 0.6476\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0134 - acc: 0.6499 - val_loss: 1.0194 - val_acc: 0.6462\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0124 - acc: 0.6505 - val_loss: 1.0235 - val_acc: 0.6445\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0118 - acc: 0.6503 - val_loss: 1.0190 - val_acc: 0.6470\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0114 - acc: 0.6510 - val_loss: 1.0197 - val_acc: 0.6465\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0109 - acc: 0.6506 - val_loss: 1.0186 - val_acc: 0.6478\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0105 - acc: 0.6511 - val_loss: 1.0177 - val_acc: 0.6485\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0098 - acc: 0.6512 - val_loss: 1.0208 - val_acc: 0.6467\n",
            "2251/2251 - 6s - loss: 1.0208 - acc: 0.6467 - 6s/epoch - 3ms/step\n",
            "Training time: 1222.9002680778503s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  1.0208451747894287\n",
            "Model accuracy:  0.6466667652130127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 3 - CNN EPOCH = 30\n"
      ],
      "metadata": {
        "id": "iSJz1DKytCpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZiub24TcbRM",
        "outputId": "fbcd05bc-ea16-433e-db0b-6fd3ea945b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.1180 - acc: 0.6183 - val_loss: 1.0688 - val_acc: 0.6309\n",
            "Epoch 2/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0536 - acc: 0.6363 - val_loss: 1.0482 - val_acc: 0.6382\n",
            "Epoch 3/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0378 - acc: 0.6414 - val_loss: 1.0389 - val_acc: 0.6394\n",
            "Epoch 4/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0308 - acc: 0.6437 - val_loss: 1.0358 - val_acc: 0.6408\n",
            "Epoch 5/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0270 - acc: 0.6455 - val_loss: 1.0308 - val_acc: 0.6416\n",
            "Epoch 6/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0242 - acc: 0.6464 - val_loss: 1.0291 - val_acc: 0.6424\n",
            "Epoch 7/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0214 - acc: 0.6478 - val_loss: 1.0304 - val_acc: 0.6424\n",
            "Epoch 8/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0197 - acc: 0.6490 - val_loss: 1.0323 - val_acc: 0.6436\n",
            "Epoch 9/30\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0187 - acc: 0.6487 - val_loss: 1.0270 - val_acc: 0.6443\n",
            "Epoch 10/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0176 - acc: 0.6488 - val_loss: 1.0274 - val_acc: 0.6449\n",
            "Epoch 11/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0168 - acc: 0.6497 - val_loss: 1.0252 - val_acc: 0.6455\n",
            "Epoch 12/30\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0158 - acc: 0.6498 - val_loss: 1.0248 - val_acc: 0.6462\n",
            "Epoch 13/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0151 - acc: 0.6503 - val_loss: 1.0240 - val_acc: 0.6463\n",
            "Epoch 14/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0145 - acc: 0.6504 - val_loss: 1.0243 - val_acc: 0.6460\n",
            "Epoch 15/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0138 - acc: 0.6506 - val_loss: 1.0238 - val_acc: 0.6461\n",
            "Epoch 16/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0132 - acc: 0.6508 - val_loss: 1.0244 - val_acc: 0.6470\n",
            "Epoch 17/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0123 - acc: 0.6508 - val_loss: 1.0234 - val_acc: 0.6456\n",
            "Epoch 18/30\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.0118 - acc: 0.6513 - val_loss: 1.0219 - val_acc: 0.6466\n",
            "Epoch 19/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0111 - acc: 0.6520 - val_loss: 1.0204 - val_acc: 0.6476\n",
            "Epoch 20/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0109 - acc: 0.6512 - val_loss: 1.0207 - val_acc: 0.6473\n",
            "Epoch 21/30\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0103 - acc: 0.6516 - val_loss: 1.0256 - val_acc: 0.6466\n",
            "Epoch 22/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0099 - acc: 0.6519 - val_loss: 1.0218 - val_acc: 0.6476\n",
            "Epoch 23/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0096 - acc: 0.6526 - val_loss: 1.0217 - val_acc: 0.6481\n",
            "Epoch 24/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0094 - acc: 0.6521 - val_loss: 1.0223 - val_acc: 0.6469\n",
            "Epoch 25/30\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0090 - acc: 0.6520 - val_loss: 1.0193 - val_acc: 0.6480\n",
            "Epoch 26/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0083 - acc: 0.6525 - val_loss: 1.0228 - val_acc: 0.6468\n",
            "Epoch 27/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0085 - acc: 0.6527 - val_loss: 1.0186 - val_acc: 0.6478\n",
            "Epoch 28/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0081 - acc: 0.6527 - val_loss: 1.0216 - val_acc: 0.6477\n",
            "Epoch 29/30\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0079 - acc: 0.6528 - val_loss: 1.0239 - val_acc: 0.6456\n",
            "Epoch 30/30\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0079 - acc: 0.6527 - val_loss: 1.0205 - val_acc: 0.6473\n",
            "2251/2251 - 7s - loss: 1.0205 - acc: 0.6473 - 7s/epoch - 3ms/step\n",
            "Training time: 1762.988395690918s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  30\n",
            "Batch Size:  64\n",
            "Model loss:  1.0205363035202026\n",
            "Model accuracy:  0.6473471522331238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Size\n"
      ],
      "metadata": {
        "id": "5__6fFjucjYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 4 - CNN BATCHSIZE = 32\n"
      ],
      "metadata": {
        "id": "rOO1k7DWtGuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size )\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQUyuSTOcejp",
        "outputId": "7abaad96-1c1a-4a93-a29f-c935c30987ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "6752/6752 [==============================] - 75s 11ms/step - loss: 1.1430 - acc: 0.6289 - val_loss: 1.0676 - val_acc: 0.6361\n",
            "Epoch 2/20\n",
            "6752/6752 [==============================] - 69s 10ms/step - loss: 1.0489 - acc: 0.6418 - val_loss: 1.0475 - val_acc: 0.6412\n",
            "Epoch 3/20\n",
            "6752/6752 [==============================] - 73s 11ms/step - loss: 1.0374 - acc: 0.6442 - val_loss: 1.0389 - val_acc: 0.6434\n",
            "Epoch 4/20\n",
            "6752/6752 [==============================] - 71s 11ms/step - loss: 1.0322 - acc: 0.6454 - val_loss: 1.0345 - val_acc: 0.6437\n",
            "Epoch 5/20\n",
            "6752/6752 [==============================] - 73s 11ms/step - loss: 1.0282 - acc: 0.6459 - val_loss: 1.0314 - val_acc: 0.6428\n",
            "Epoch 6/20\n",
            "6752/6752 [==============================] - 69s 10ms/step - loss: 1.0254 - acc: 0.6461 - val_loss: 1.0286 - val_acc: 0.6449\n",
            "Epoch 7/20\n",
            "6752/6752 [==============================] - 75s 11ms/step - loss: 1.0237 - acc: 0.6467 - val_loss: 1.0270 - val_acc: 0.6445\n",
            "Epoch 8/20\n",
            "6752/6752 [==============================] - 73s 11ms/step - loss: 1.0217 - acc: 0.6473 - val_loss: 1.0278 - val_acc: 0.6444\n",
            "Epoch 9/20\n",
            "6752/6752 [==============================] - 72s 11ms/step - loss: 1.0211 - acc: 0.6473 - val_loss: 1.0308 - val_acc: 0.6452\n",
            "Epoch 10/20\n",
            "6752/6752 [==============================] - 76s 11ms/step - loss: 1.0209 - acc: 0.6481 - val_loss: 1.0303 - val_acc: 0.6462\n",
            "Epoch 11/20\n",
            "6752/6752 [==============================] - 75s 11ms/step - loss: 1.0204 - acc: 0.6486 - val_loss: 1.0314 - val_acc: 0.6418\n",
            "Epoch 12/20\n",
            "6752/6752 [==============================] - 79s 12ms/step - loss: 1.0206 - acc: 0.6489 - val_loss: 1.0251 - val_acc: 0.6456\n",
            "Epoch 13/20\n",
            "6752/6752 [==============================] - 81s 12ms/step - loss: 1.0201 - acc: 0.6489 - val_loss: 1.0284 - val_acc: 0.6473\n",
            "Epoch 14/20\n",
            "6752/6752 [==============================] - 73s 11ms/step - loss: 1.0209 - acc: 0.6491 - val_loss: 1.0322 - val_acc: 0.6452\n",
            "Epoch 15/20\n",
            "6752/6752 [==============================] - 75s 11ms/step - loss: 1.0215 - acc: 0.6494 - val_loss: 1.0315 - val_acc: 0.6475\n",
            "Epoch 16/20\n",
            "6752/6752 [==============================] - 71s 11ms/step - loss: 1.0224 - acc: 0.6493 - val_loss: 1.0395 - val_acc: 0.6431\n",
            "Epoch 17/20\n",
            "6752/6752 [==============================] - 74s 11ms/step - loss: 1.0238 - acc: 0.6489 - val_loss: 1.0296 - val_acc: 0.6473\n",
            "Epoch 18/20\n",
            "6752/6752 [==============================] - 80s 12ms/step - loss: 1.0254 - acc: 0.6488 - val_loss: 1.0387 - val_acc: 0.6430\n",
            "Epoch 19/20\n",
            "6752/6752 [==============================] - 76s 11ms/step - loss: 1.0275 - acc: 0.6481 - val_loss: 1.0373 - val_acc: 0.6447\n",
            "Epoch 20/20\n",
            "6752/6752 [==============================] - 75s 11ms/step - loss: 1.0283 - acc: 0.6482 - val_loss: 1.0332 - val_acc: 0.6461\n",
            "2251/2251 - 6s - loss: 1.0332 - acc: 0.6461 - 6s/epoch - 2ms/step\n",
            "Training time: 1523.0283484458923s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  32\n",
            "Model loss:  1.0332202911376953\n",
            "Model accuracy:  0.6461390852928162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 5 - CNN BATCHSIZE = 64\n"
      ],
      "metadata": {
        "id": "jiJMbbpttM7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlAqnKkBce7u",
        "outputId": "7bbbfe63-3fc6-49ef-803f-d6f2729fabfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.1117 - acc: 0.6202 - val_loss: 1.0669 - val_acc: 0.6315\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0510 - acc: 0.6385 - val_loss: 1.0488 - val_acc: 0.6393\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0379 - acc: 0.6431 - val_loss: 1.0382 - val_acc: 0.6416\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0315 - acc: 0.6450 - val_loss: 1.0369 - val_acc: 0.6429\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.0278 - acc: 0.6460 - val_loss: 1.0451 - val_acc: 0.6388\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0256 - acc: 0.6462 - val_loss: 1.0296 - val_acc: 0.6450\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0235 - acc: 0.6474 - val_loss: 1.0275 - val_acc: 0.6442\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0220 - acc: 0.6475 - val_loss: 1.0281 - val_acc: 0.6451\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0207 - acc: 0.6483 - val_loss: 1.0423 - val_acc: 0.6399\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0200 - acc: 0.6489 - val_loss: 1.0263 - val_acc: 0.6451\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 64s 19ms/step - loss: 1.0196 - acc: 0.6484 - val_loss: 1.0265 - val_acc: 0.6445\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0190 - acc: 0.6489 - val_loss: 1.0285 - val_acc: 0.6456\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0185 - acc: 0.6488 - val_loss: 1.0245 - val_acc: 0.6453\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0180 - acc: 0.6492 - val_loss: 1.0257 - val_acc: 0.6447\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 64s 19ms/step - loss: 1.0176 - acc: 0.6492 - val_loss: 1.0267 - val_acc: 0.6452\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0169 - acc: 0.6494 - val_loss: 1.0244 - val_acc: 0.6453\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0164 - acc: 0.6495 - val_loss: 1.0241 - val_acc: 0.6463\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0164 - acc: 0.6495 - val_loss: 1.0246 - val_acc: 0.6454\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0162 - acc: 0.6492 - val_loss: 1.0260 - val_acc: 0.6459\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.0158 - acc: 0.6496 - val_loss: 1.0267 - val_acc: 0.6463\n",
            "2251/2251 - 7s - loss: 1.0267 - acc: 0.6463 - 7s/epoch - 3ms/step\n",
            "Training time: 1187.897717475891s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  1.0266718864440918\n",
            "Model accuracy:  0.6463196277618408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 6 - CNN BATCHSIZE = 128\n"
      ],
      "metadata": {
        "id": "9xVq3NvVtO0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size )\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xw0H3aucfFO",
        "outputId": "e9dac9bb-bb77-4f9e-b535-97219b3bf6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1688/1688 [==============================] - 57s 33ms/step - loss: 1.1223 - acc: 0.6162 - val_loss: 1.0831 - val_acc: 0.6247\n",
            "Epoch 2/20\n",
            "1688/1688 [==============================] - 54s 32ms/step - loss: 1.0680 - acc: 0.6327 - val_loss: 1.0639 - val_acc: 0.6324\n",
            "Epoch 3/20\n",
            "1688/1688 [==============================] - 55s 32ms/step - loss: 1.0530 - acc: 0.6382 - val_loss: 1.0490 - val_acc: 0.6377\n",
            "Epoch 4/20\n",
            "1688/1688 [==============================] - 55s 33ms/step - loss: 1.0397 - acc: 0.6429 - val_loss: 1.0406 - val_acc: 0.6412\n",
            "Epoch 5/20\n",
            "1688/1688 [==============================] - 55s 33ms/step - loss: 1.0314 - acc: 0.6458 - val_loss: 1.0340 - val_acc: 0.6430\n",
            "Epoch 6/20\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 1.0267 - acc: 0.6467 - val_loss: 1.0306 - val_acc: 0.6429\n",
            "Epoch 7/20\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 1.0234 - acc: 0.6475 - val_loss: 1.0294 - val_acc: 0.6442\n",
            "Epoch 8/20\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 1.0209 - acc: 0.6482 - val_loss: 1.0269 - val_acc: 0.6450\n",
            "Epoch 9/20\n",
            "1688/1688 [==============================] - 54s 32ms/step - loss: 1.0193 - acc: 0.6488 - val_loss: 1.0332 - val_acc: 0.6428\n",
            "Epoch 10/20\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 1.0179 - acc: 0.6495 - val_loss: 1.0254 - val_acc: 0.6458\n",
            "Epoch 11/20\n",
            "1688/1688 [==============================] - 55s 33ms/step - loss: 1.0162 - acc: 0.6499 - val_loss: 1.0261 - val_acc: 0.6448\n",
            "Epoch 12/20\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 1.0155 - acc: 0.6498 - val_loss: 1.0242 - val_acc: 0.6453\n",
            "Epoch 13/20\n",
            "1688/1688 [==============================] - 57s 33ms/step - loss: 1.0142 - acc: 0.6505 - val_loss: 1.0212 - val_acc: 0.6475\n",
            "Epoch 14/20\n",
            "1688/1688 [==============================] - 54s 32ms/step - loss: 1.0133 - acc: 0.6504 - val_loss: 1.0222 - val_acc: 0.6466\n",
            "Epoch 15/20\n",
            "1688/1688 [==============================] - 57s 34ms/step - loss: 1.0125 - acc: 0.6503 - val_loss: 1.0258 - val_acc: 0.6449\n",
            "Epoch 16/20\n",
            "1688/1688 [==============================] - 55s 32ms/step - loss: 1.0118 - acc: 0.6509 - val_loss: 1.0223 - val_acc: 0.6459\n",
            "Epoch 17/20\n",
            "1688/1688 [==============================] - 56s 33ms/step - loss: 1.0112 - acc: 0.6510 - val_loss: 1.0227 - val_acc: 0.6460\n",
            "Epoch 18/20\n",
            "1688/1688 [==============================] - 55s 32ms/step - loss: 1.0109 - acc: 0.6514 - val_loss: 1.0200 - val_acc: 0.6480\n",
            "Epoch 19/20\n",
            "1688/1688 [==============================] - 54s 32ms/step - loss: 1.0105 - acc: 0.6513 - val_loss: 1.0244 - val_acc: 0.6468\n",
            "Epoch 20/20\n",
            "1688/1688 [==============================] - 55s 32ms/step - loss: 1.0100 - acc: 0.6515 - val_loss: 1.0190 - val_acc: 0.6468\n",
            "2251/2251 - 7s - loss: 1.0190 - acc: 0.6468 - 7s/epoch - 3ms/step\n",
            "Training time: 1110.6119360923767s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  128\n",
            "Model loss:  1.0190237760543823\n",
            "Model accuracy:  0.6468333601951599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocab Size\n"
      ],
      "metadata": {
        "id": "VBaA5v7BcuoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 7 - CNN max_vocab_size = 1000\n"
      ],
      "metadata": {
        "id": "DYbtB1kqtRY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 1000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size )\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TIEMLn3Z-gD",
        "outputId": "b00adb76-8752-4850-a3ac-376e62beb10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 60s 18ms/step - loss: 1.1132 - acc: 0.6205 - val_loss: 1.0716 - val_acc: 0.6315\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 65s 19ms/step - loss: 1.0563 - acc: 0.6368 - val_loss: 1.0539 - val_acc: 0.6359\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0447 - acc: 0.6401 - val_loss: 1.0462 - val_acc: 0.6373\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 65s 19ms/step - loss: 1.0388 - acc: 0.6408 - val_loss: 1.0401 - val_acc: 0.6392\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 64s 19ms/step - loss: 1.0350 - acc: 0.6418 - val_loss: 1.0357 - val_acc: 0.6413\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0321 - acc: 0.6434 - val_loss: 1.0402 - val_acc: 0.6389\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0292 - acc: 0.6439 - val_loss: 1.0307 - val_acc: 0.6428\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0267 - acc: 0.6446 - val_loss: 1.0285 - val_acc: 0.6433\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0245 - acc: 0.6461 - val_loss: 1.0377 - val_acc: 0.6429\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 65s 19ms/step - loss: 1.0228 - acc: 0.6466 - val_loss: 1.0272 - val_acc: 0.6445\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0215 - acc: 0.6467 - val_loss: 1.0268 - val_acc: 0.6440\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0207 - acc: 0.6476 - val_loss: 1.0268 - val_acc: 0.6440\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0196 - acc: 0.6473 - val_loss: 1.0240 - val_acc: 0.6453\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0189 - acc: 0.6478 - val_loss: 1.0295 - val_acc: 0.6433\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0185 - acc: 0.6478 - val_loss: 1.0236 - val_acc: 0.6460\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0183 - acc: 0.6480 - val_loss: 1.0304 - val_acc: 0.6444\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0181 - acc: 0.6481 - val_loss: 1.0265 - val_acc: 0.6460\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 61s 18ms/step - loss: 1.0180 - acc: 0.6480 - val_loss: 1.0266 - val_acc: 0.6458\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0185 - acc: 0.6481 - val_loss: 1.0387 - val_acc: 0.6411\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0183 - acc: 0.6484 - val_loss: 1.0285 - val_acc: 0.6443\n",
            "2251/2251 - 7s - loss: 1.0285 - acc: 0.6443 - 7s/epoch - 3ms/step\n",
            "Training time: 1222.9312875270844s\n",
            "Maximum number of words in tokenizer:  1000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  1.0284628868103027\n",
            "Model accuracy:  0.6442645192146301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 8 - CNN max_vocab_size = 2000\n"
      ],
      "metadata": {
        "id": "G_XLgG_dtWBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size )\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFkH7aNodBhZ",
        "outputId": "24eb7dac-ff33-4719-abf5-99541ba2d6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.1002 - acc: 0.6251 - val_loss: 1.0665 - val_acc: 0.6332\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0549 - acc: 0.6379 - val_loss: 1.0567 - val_acc: 0.6353\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0443 - acc: 0.6410 - val_loss: 1.0465 - val_acc: 0.6376\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 56s 17ms/step - loss: 1.0375 - acc: 0.6421 - val_loss: 1.0409 - val_acc: 0.6396\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0314 - acc: 0.6448 - val_loss: 1.0320 - val_acc: 0.6432\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0264 - acc: 0.6463 - val_loss: 1.0288 - val_acc: 0.6441\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0223 - acc: 0.6472 - val_loss: 1.0338 - val_acc: 0.6432\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0193 - acc: 0.6483 - val_loss: 1.0302 - val_acc: 0.6433\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0173 - acc: 0.6489 - val_loss: 1.0261 - val_acc: 0.6439\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0160 - acc: 0.6493 - val_loss: 1.0364 - val_acc: 0.6425\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 56s 17ms/step - loss: 1.0149 - acc: 0.6500 - val_loss: 1.0249 - val_acc: 0.6449\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0144 - acc: 0.6505 - val_loss: 1.0215 - val_acc: 0.6454\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0138 - acc: 0.6502 - val_loss: 1.0199 - val_acc: 0.6465\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 62s 18ms/step - loss: 1.0133 - acc: 0.6505 - val_loss: 1.0230 - val_acc: 0.6465\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 56s 17ms/step - loss: 1.0132 - acc: 0.6507 - val_loss: 1.0241 - val_acc: 0.6461\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0130 - acc: 0.6507 - val_loss: 1.0202 - val_acc: 0.6462\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0127 - acc: 0.6508 - val_loss: 1.0183 - val_acc: 0.6473\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0130 - acc: 0.6508 - val_loss: 1.0355 - val_acc: 0.6428\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0126 - acc: 0.6506 - val_loss: 1.0307 - val_acc: 0.6485\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0130 - acc: 0.6509 - val_loss: 1.0201 - val_acc: 0.6459\n",
            "2251/2251 - 5s - loss: 1.0201 - acc: 0.6459 - 5s/epoch - 2ms/step\n",
            "Training time: 1173.8979260921478s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  1.020060420036316\n",
            "Model accuracy:  0.6458752751350403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 9 - CNN max_vocab_size = 3000\n"
      ],
      "metadata": {
        "id": "KJ8lpHv4tYfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 3000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "CNN_model = Sequential()\n",
        "CNN_model.add(layers.Embedding(max_num_words, 50))\n",
        "CNN_model.add(layers.Conv1D(5, 4, activation='relu'))\n",
        "CNN_model.add(layers.MaxPooling1D(10))\n",
        "\n",
        "\n",
        "CNN_model.add(layers.Conv1D(10, 8, activation='relu'))\n",
        "CNN_model.add(layers.GlobalMaxPooling1D())\n",
        "\n",
        "CNN_model.add(layers.Dense(6,activation='softmax'))\n",
        "CNN_model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "start = time.time()\n",
        "CNN_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size )\n",
        "stop = time.time()\n",
        "\n",
        "\n",
        "test_loss, test_acc = CNN_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INLtadJRdCt0",
        "outputId": "568d69fa-cd51-4302-9a66-7c3a99dafee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.1032 - acc: 0.6241 - val_loss: 1.0621 - val_acc: 0.6325\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0490 - acc: 0.6382 - val_loss: 1.0543 - val_acc: 0.6366\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0364 - acc: 0.6418 - val_loss: 1.0363 - val_acc: 0.6413\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0289 - acc: 0.6442 - val_loss: 1.0300 - val_acc: 0.6427\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 62s 18ms/step - loss: 1.0242 - acc: 0.6457 - val_loss: 1.0297 - val_acc: 0.6434\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0208 - acc: 0.6467 - val_loss: 1.0283 - val_acc: 0.6433\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0187 - acc: 0.6467 - val_loss: 1.0296 - val_acc: 0.6454\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0176 - acc: 0.6471 - val_loss: 1.0247 - val_acc: 0.6457\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 59s 17ms/step - loss: 1.0169 - acc: 0.6475 - val_loss: 1.0211 - val_acc: 0.6458\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0160 - acc: 0.6480 - val_loss: 1.0239 - val_acc: 0.6456\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0153 - acc: 0.6479 - val_loss: 1.0215 - val_acc: 0.6468\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 65s 19ms/step - loss: 1.0144 - acc: 0.6487 - val_loss: 1.0243 - val_acc: 0.6460\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0141 - acc: 0.6482 - val_loss: 1.0253 - val_acc: 0.6445\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0138 - acc: 0.6485 - val_loss: 1.0221 - val_acc: 0.6464\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 57s 17ms/step - loss: 1.0131 - acc: 0.6486 - val_loss: 1.0265 - val_acc: 0.6462\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 64s 19ms/step - loss: 1.0126 - acc: 0.6491 - val_loss: 1.0247 - val_acc: 0.6466\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0123 - acc: 0.6488 - val_loss: 1.0242 - val_acc: 0.6470\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 63s 19ms/step - loss: 1.0118 - acc: 0.6490 - val_loss: 1.0213 - val_acc: 0.6468\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 58s 17ms/step - loss: 1.0118 - acc: 0.6498 - val_loss: 1.0379 - val_acc: 0.6430\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 59s 18ms/step - loss: 1.0115 - acc: 0.6490 - val_loss: 1.0214 - val_acc: 0.6447\n",
            "2251/2251 - 6s - loss: 1.0214 - acc: 0.6447 - 6s/epoch - 2ms/step\n",
            "Training time: 1199.7053053379059s\n",
            "Maximum number of words in tokenizer:  3000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  1.0213563442230225\n",
            "Model accuracy:  0.6446533203125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model Experiments\n",
        "Epoch"
      ],
      "metadata": {
        "id": "A4fz_XCRcbc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 1 - LSTM epoch = 10  \n"
      ],
      "metadata": {
        "id": "5xTKHhERtawI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmwF-lau3cem",
        "outputId": "0cb7965f-2992-4db7-f0e0-5ed80953a668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/10\n",
            "3376/3376 [==============================] - 279s 82ms/step - loss: 1.1311 - accuracy: 0.6119 - val_loss: 1.0961 - val_accuracy: 0.6173\n",
            "Epoch 2/10\n",
            "3376/3376 [==============================] - 272s 81ms/step - loss: 1.0850 - accuracy: 0.6239 - val_loss: 1.0689 - val_accuracy: 0.6304\n",
            "Epoch 3/10\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 1.0556 - accuracy: 0.6329 - val_loss: 1.0704 - val_accuracy: 0.6354\n",
            "Epoch 4/10\n",
            "3376/3376 [==============================] - 271s 80ms/step - loss: 1.0342 - accuracy: 0.6391 - val_loss: 1.0229 - val_accuracy: 0.6423\n",
            "Epoch 5/10\n",
            "3376/3376 [==============================] - 274s 81ms/step - loss: 1.0151 - accuracy: 0.6446 - val_loss: 1.0108 - val_accuracy: 0.6442\n",
            "Epoch 6/10\n",
            "3376/3376 [==============================] - 274s 81ms/step - loss: 1.0015 - accuracy: 0.6489 - val_loss: 1.0154 - val_accuracy: 0.6388\n",
            "Epoch 7/10\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9909 - accuracy: 0.6520 - val_loss: 0.9918 - val_accuracy: 0.6510\n",
            "Epoch 8/10\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 0.9838 - accuracy: 0.6545 - val_loss: 0.9838 - val_accuracy: 0.6532\n",
            "Epoch 9/10\n",
            "3376/3376 [==============================] - 275s 81ms/step - loss: 0.9781 - accuracy: 0.6558 - val_loss: 0.9801 - val_accuracy: 0.6540\n",
            "Epoch 10/10\n",
            "3376/3376 [==============================] - 271s 80ms/step - loss: 0.9732 - accuracy: 0.6576 - val_loss: 0.9788 - val_accuracy: 0.6551\n",
            "2251/2251 - 28s - loss: 0.9788 - accuracy: 0.6551 - 28s/epoch - 12ms/step\n",
            "Training time: 2739.2226009368896s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  10\n",
            "Batch Size:  64\n",
            "Model loss:  0.978752613067627\n",
            "Model accuracy:  0.6550675630569458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 2 - LSTM epoch = 20  \n"
      ],
      "metadata": {
        "id": "X5CBqPP8thsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ivgCw0dONS",
        "outputId": "249cf14e-cf03-40a9-801f-479ba040d010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 277s 81ms/step - loss: 1.1312 - accuracy: 0.6115 - val_loss: 1.1002 - val_accuracy: 0.6162\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0874 - accuracy: 0.6238 - val_loss: 1.0690 - val_accuracy: 0.6297\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 274s 81ms/step - loss: 1.0576 - accuracy: 0.6341 - val_loss: 1.0441 - val_accuracy: 0.6388\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 275s 81ms/step - loss: 1.0344 - accuracy: 0.6410 - val_loss: 1.0276 - val_accuracy: 0.6435\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 1.0182 - accuracy: 0.6459 - val_loss: 1.0081 - val_accuracy: 0.6461\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 1.0065 - accuracy: 0.6486 - val_loss: 1.0016 - val_accuracy: 0.6487\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9985 - accuracy: 0.6505 - val_loss: 0.9999 - val_accuracy: 0.6490\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 0.9916 - accuracy: 0.6533 - val_loss: 0.9918 - val_accuracy: 0.6488\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9858 - accuracy: 0.6550 - val_loss: 0.9882 - val_accuracy: 0.6508\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 275s 81ms/step - loss: 0.9799 - accuracy: 0.6562 - val_loss: 0.9832 - val_accuracy: 0.6541\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9750 - accuracy: 0.6577 - val_loss: 0.9764 - val_accuracy: 0.6565\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 0.9702 - accuracy: 0.6600 - val_loss: 0.9703 - val_accuracy: 0.6587\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9657 - accuracy: 0.6615 - val_loss: 0.9751 - val_accuracy: 0.6566\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9617 - accuracy: 0.6625 - val_loss: 0.9668 - val_accuracy: 0.6606\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 275s 81ms/step - loss: 0.9584 - accuracy: 0.6634 - val_loss: 0.9644 - val_accuracy: 0.6622\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9552 - accuracy: 0.6650 - val_loss: 0.9594 - val_accuracy: 0.6637\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 275s 82ms/step - loss: 0.9525 - accuracy: 0.6662 - val_loss: 0.9613 - val_accuracy: 0.6651\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 272s 81ms/step - loss: 0.9501 - accuracy: 0.6673 - val_loss: 0.9594 - val_accuracy: 0.6641\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 274s 81ms/step - loss: 0.9476 - accuracy: 0.6677 - val_loss: 0.9588 - val_accuracy: 0.6638\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 273s 81ms/step - loss: 0.9455 - accuracy: 0.6693 - val_loss: 0.9502 - val_accuracy: 0.6673\n",
            "2251/2251 - 28s - loss: 0.9502 - accuracy: 0.6673 - 28s/epoch - 12ms/step\n",
            "Training time: 5545.580643892288s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  0.9502121806144714\n",
            "Model accuracy:  0.6673007607460022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 3 - LSTM epoch = 30  \n"
      ],
      "metadata": {
        "id": "WvqGXqyTtj9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2) # verbose shows number of epochs in training\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLSy6x1xdP08",
        "outputId": "4c8aa4ac-20b0-4efb-c08b-14c5f767b51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/30\n",
            "3376/3376 [==============================] - 283s 83ms/step - loss: 1.1351 - accuracy: 0.6114 - val_loss: 1.1103 - val_accuracy: 0.6109\n",
            "Epoch 2/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 1.0974 - accuracy: 0.6215 - val_loss: 1.0915 - val_accuracy: 0.6223\n",
            "Epoch 3/30\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 1.0752 - accuracy: 0.6279 - val_loss: 1.0747 - val_accuracy: 0.6264\n",
            "Epoch 4/30\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 1.0535 - accuracy: 0.6337 - val_loss: 1.0405 - val_accuracy: 0.6363\n",
            "Epoch 5/30\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 1.0287 - accuracy: 0.6412 - val_loss: 1.0247 - val_accuracy: 0.6386\n",
            "Epoch 6/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 1.0101 - accuracy: 0.6471 - val_loss: 1.0107 - val_accuracy: 0.6438\n",
            "Epoch 7/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9976 - accuracy: 0.6511 - val_loss: 0.9988 - val_accuracy: 0.6500\n",
            "Epoch 8/30\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9878 - accuracy: 0.6549 - val_loss: 0.9975 - val_accuracy: 0.6522\n",
            "Epoch 9/30\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9793 - accuracy: 0.6580 - val_loss: 0.9820 - val_accuracy: 0.6573\n",
            "Epoch 10/30\n",
            "3376/3376 [==============================] - 275s 81ms/step - loss: 0.9721 - accuracy: 0.6605 - val_loss: 0.9771 - val_accuracy: 0.6589\n",
            "Epoch 11/30\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9654 - accuracy: 0.6626 - val_loss: 0.9708 - val_accuracy: 0.6598\n",
            "Epoch 12/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9600 - accuracy: 0.6642 - val_loss: 0.9650 - val_accuracy: 0.6626\n",
            "Epoch 13/30\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9554 - accuracy: 0.6658 - val_loss: 0.9749 - val_accuracy: 0.6571\n",
            "Epoch 14/30\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9519 - accuracy: 0.6669 - val_loss: 0.9791 - val_accuracy: 0.6556\n",
            "Epoch 15/30\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9495 - accuracy: 0.6678 - val_loss: 0.9586 - val_accuracy: 0.6656\n",
            "Epoch 16/30\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9469 - accuracy: 0.6687 - val_loss: 0.9563 - val_accuracy: 0.6652\n",
            "Epoch 17/30\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9447 - accuracy: 0.6694 - val_loss: 0.9601 - val_accuracy: 0.6634\n",
            "Epoch 18/30\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9428 - accuracy: 0.6701 - val_loss: 0.9512 - val_accuracy: 0.6672\n",
            "Epoch 19/30\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9411 - accuracy: 0.6706 - val_loss: 0.9493 - val_accuracy: 0.6683\n",
            "Epoch 20/30\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9396 - accuracy: 0.6709 - val_loss: 0.9527 - val_accuracy: 0.6666\n",
            "Epoch 21/30\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9383 - accuracy: 0.6710 - val_loss: 0.9468 - val_accuracy: 0.6684\n",
            "Epoch 22/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9370 - accuracy: 0.6723 - val_loss: 0.9501 - val_accuracy: 0.6678\n",
            "Epoch 23/30\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9359 - accuracy: 0.6724 - val_loss: 0.9486 - val_accuracy: 0.6688\n",
            "Epoch 24/30\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9344 - accuracy: 0.6732 - val_loss: 0.9446 - val_accuracy: 0.6690\n",
            "Epoch 25/30\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9333 - accuracy: 0.6736 - val_loss: 0.9462 - val_accuracy: 0.6700\n",
            "Epoch 26/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9326 - accuracy: 0.6741 - val_loss: 0.9477 - val_accuracy: 0.6701\n",
            "Epoch 27/30\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9318 - accuracy: 0.6745 - val_loss: 0.9459 - val_accuracy: 0.6695\n",
            "Epoch 28/30\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9306 - accuracy: 0.6745 - val_loss: 0.9413 - val_accuracy: 0.6710\n",
            "Epoch 29/30\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9299 - accuracy: 0.6749 - val_loss: 0.9429 - val_accuracy: 0.6692\n",
            "Epoch 30/30\n",
            "3376/3376 [==============================] - 282s 83ms/step - loss: 0.9292 - accuracy: 0.6750 - val_loss: 0.9398 - val_accuracy: 0.6722\n",
            "2251/2251 - 28s - loss: 0.9398 - accuracy: 0.6722 - 28s/epoch - 12ms/step\n",
            "Training time: 8362.207289218903s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  30\n",
            "Batch Size:  64\n",
            "Model loss:  0.939788818359375\n",
            "Model accuracy:  0.6721746325492859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max Vocab"
      ],
      "metadata": {
        "id": "FWh9XUyBdRo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 4 - LSTM max_vocab_size = 1000"
      ],
      "metadata": {
        "id": "LBSiTGpctlzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 1000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-4P0oiBdTJQ",
        "outputId": "7fbeeb23-6ba7-47e6-8da6-035e233e30cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 281s 82ms/step - loss: 1.1279 - accuracy: 0.6133 - val_loss: 1.1206 - val_accuracy: 0.6197\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 1.0884 - accuracy: 0.6258 - val_loss: 1.0809 - val_accuracy: 0.6246\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 1.0629 - accuracy: 0.6335 - val_loss: 1.0526 - val_accuracy: 0.6346\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 282s 83ms/step - loss: 1.0415 - accuracy: 0.6386 - val_loss: 1.0492 - val_accuracy: 0.6318\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0271 - accuracy: 0.6426 - val_loss: 1.0252 - val_accuracy: 0.6415\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 1.0139 - accuracy: 0.6455 - val_loss: 1.0173 - val_accuracy: 0.6427\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0025 - accuracy: 0.6485 - val_loss: 1.0069 - val_accuracy: 0.6457\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 0.9941 - accuracy: 0.6510 - val_loss: 0.9970 - val_accuracy: 0.6492\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 0.9866 - accuracy: 0.6537 - val_loss: 0.9935 - val_accuracy: 0.6496\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9807 - accuracy: 0.6559 - val_loss: 0.9819 - val_accuracy: 0.6544\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9758 - accuracy: 0.6576 - val_loss: 0.9875 - val_accuracy: 0.6530\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9712 - accuracy: 0.6590 - val_loss: 0.9800 - val_accuracy: 0.6550\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9673 - accuracy: 0.6609 - val_loss: 0.9817 - val_accuracy: 0.6550\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 276s 82ms/step - loss: 0.9638 - accuracy: 0.6619 - val_loss: 0.9696 - val_accuracy: 0.6594\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9606 - accuracy: 0.6635 - val_loss: 0.9653 - val_accuracy: 0.6621\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9575 - accuracy: 0.6648 - val_loss: 0.9625 - val_accuracy: 0.6636\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9549 - accuracy: 0.6661 - val_loss: 0.9586 - val_accuracy: 0.6647\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9526 - accuracy: 0.6667 - val_loss: 0.9610 - val_accuracy: 0.6641\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9507 - accuracy: 0.6679 - val_loss: 0.9568 - val_accuracy: 0.6652\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9487 - accuracy: 0.6685 - val_loss: 0.9655 - val_accuracy: 0.6647\n",
            "2251/2251 - 28s - loss: 0.9655 - accuracy: 0.6647 - 28s/epoch - 13ms/step\n",
            "Training time: 5590.644472360611s\n",
            "Maximum number of words in tokenizer:  1000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  0.9655449986457825\n",
            "Model accuracy:  0.6646902561187744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 5 - LSTM max_vocab_size = 2000"
      ],
      "metadata": {
        "id": "5tCHplGAtpUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ClYuVzadTnq",
        "outputId": "9ab250e6-3930-4020-c804-78c90e9938d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 285s 83ms/step - loss: 1.1348 - accuracy: 0.6094 - val_loss: 1.1160 - val_accuracy: 0.6070\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 1.1026 - accuracy: 0.6184 - val_loss: 1.0961 - val_accuracy: 0.6178\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 284s 84ms/step - loss: 1.0853 - accuracy: 0.6244 - val_loss: 1.0732 - val_accuracy: 0.6266\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 285s 84ms/step - loss: 1.0709 - accuracy: 0.6293 - val_loss: 1.0666 - val_accuracy: 0.6278\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 1.0573 - accuracy: 0.6323 - val_loss: 1.0480 - val_accuracy: 0.6318\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 284s 84ms/step - loss: 1.0437 - accuracy: 0.6343 - val_loss: 1.0402 - val_accuracy: 0.6337\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 1.0306 - accuracy: 0.6385 - val_loss: 1.0312 - val_accuracy: 0.6386\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 282s 83ms/step - loss: 1.0215 - accuracy: 0.6415 - val_loss: 1.0245 - val_accuracy: 0.6376\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 1.0134 - accuracy: 0.6441 - val_loss: 1.0102 - val_accuracy: 0.6438\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 1.0069 - accuracy: 0.6464 - val_loss: 1.0029 - val_accuracy: 0.6471\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 1.0003 - accuracy: 0.6491 - val_loss: 0.9971 - val_accuracy: 0.6488\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9950 - accuracy: 0.6512 - val_loss: 1.0019 - val_accuracy: 0.6498\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9893 - accuracy: 0.6539 - val_loss: 0.9883 - val_accuracy: 0.6544\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9852 - accuracy: 0.6552 - val_loss: 0.9833 - val_accuracy: 0.6558\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9805 - accuracy: 0.6565 - val_loss: 0.9803 - val_accuracy: 0.6567\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9769 - accuracy: 0.6576 - val_loss: 0.9815 - val_accuracy: 0.6572\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9736 - accuracy: 0.6588 - val_loss: 0.9813 - val_accuracy: 0.6565\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 0.9707 - accuracy: 0.6594 - val_loss: 0.9745 - val_accuracy: 0.6583\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9685 - accuracy: 0.6603 - val_loss: 0.9738 - val_accuracy: 0.6584\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9658 - accuracy: 0.6608 - val_loss: 0.9698 - val_accuracy: 0.6595\n",
            "2251/2251 - 29s - loss: 0.9698 - accuracy: 0.6595 - 29s/epoch - 13ms/step\n",
            "Training time: 5665.510201931s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  0.9697689414024353\n",
            "Model accuracy:  0.6595248579978943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 6 - LSTM max_vocab_size = 3000"
      ],
      "metadata": {
        "id": "vXHMaUw2trU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 3000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeNHysKFdT2s",
        "outputId": "d75771fa-27d2-4e15-ef58-462a4818452a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 285s 84ms/step - loss: 1.1344 - accuracy: 0.6125 - val_loss: 1.1168 - val_accuracy: 0.6160\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 285s 84ms/step - loss: 1.0968 - accuracy: 0.6230 - val_loss: 1.0969 - val_accuracy: 0.6248\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 285s 84ms/step - loss: 1.0785 - accuracy: 0.6273 - val_loss: 1.0710 - val_accuracy: 0.6276\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 1.0539 - accuracy: 0.6341 - val_loss: 1.0494 - val_accuracy: 0.6334\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 287s 85ms/step - loss: 1.0353 - accuracy: 0.6381 - val_loss: 1.0343 - val_accuracy: 0.6337\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 1.0204 - accuracy: 0.6418 - val_loss: 1.0146 - val_accuracy: 0.6425\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 1.0085 - accuracy: 0.6453 - val_loss: 1.0177 - val_accuracy: 0.6395\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9995 - accuracy: 0.6488 - val_loss: 1.0084 - val_accuracy: 0.6488\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 0.9920 - accuracy: 0.6515 - val_loss: 0.9959 - val_accuracy: 0.6510\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9850 - accuracy: 0.6540 - val_loss: 0.9903 - val_accuracy: 0.6527\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9788 - accuracy: 0.6570 - val_loss: 0.9808 - val_accuracy: 0.6555\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 284s 84ms/step - loss: 0.9742 - accuracy: 0.6588 - val_loss: 0.9763 - val_accuracy: 0.6572\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9697 - accuracy: 0.6603 - val_loss: 0.9773 - val_accuracy: 0.6575\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 284s 84ms/step - loss: 0.9657 - accuracy: 0.6616 - val_loss: 0.9812 - val_accuracy: 0.6537\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9620 - accuracy: 0.6634 - val_loss: 0.9722 - val_accuracy: 0.6594\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 283s 84ms/step - loss: 0.9591 - accuracy: 0.6641 - val_loss: 0.9723 - val_accuracy: 0.6568\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 0.9567 - accuracy: 0.6649 - val_loss: 0.9661 - val_accuracy: 0.6626\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9545 - accuracy: 0.6660 - val_loss: 0.9632 - val_accuracy: 0.6629\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 284s 84ms/step - loss: 0.9518 - accuracy: 0.6671 - val_loss: 0.9636 - val_accuracy: 0.6614\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 282s 84ms/step - loss: 0.9502 - accuracy: 0.6674 - val_loss: 0.9569 - val_accuracy: 0.6648\n",
            "2251/2251 - 28s - loss: 0.9569 - accuracy: 0.6648 - 28s/epoch - 13ms/step\n",
            "Training time: 5664.465989112854s\n",
            "Maximum number of words in tokenizer:  3000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  0.9569283723831177\n",
            "Model accuracy:  0.6647596955299377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Size\n"
      ],
      "metadata": {
        "id": "RWv58to_dWEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 7 - LSTM batch_size = 32"
      ],
      "metadata": {
        "id": "1hFEyLb9tto4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGiS7CDddXKt",
        "outputId": "6d45da1a-4d04-4256-cadc-7279f66473a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "6752/6752 [==============================] - 501s 74ms/step - loss: 1.1163 - accuracy: 0.6144 - val_loss: 1.0819 - val_accuracy: 0.6243\n",
            "Epoch 2/20\n",
            "6752/6752 [==============================] - 515s 76ms/step - loss: 1.0732 - accuracy: 0.6291 - val_loss: 1.0630 - val_accuracy: 0.6316\n",
            "Epoch 3/20\n",
            "6752/6752 [==============================] - 505s 75ms/step - loss: 1.0526 - accuracy: 0.6350 - val_loss: 1.0634 - val_accuracy: 0.6356\n",
            "Epoch 4/20\n",
            "6752/6752 [==============================] - 505s 75ms/step - loss: 1.0324 - accuracy: 0.6405 - val_loss: 1.0223 - val_accuracy: 0.6399\n",
            "Epoch 5/20\n",
            "6752/6752 [==============================] - 519s 77ms/step - loss: 1.0147 - accuracy: 0.6460 - val_loss: 1.0069 - val_accuracy: 0.6447\n",
            "Epoch 6/20\n",
            "6752/6752 [==============================] - 513s 76ms/step - loss: 1.0016 - accuracy: 0.6500 - val_loss: 0.9956 - val_accuracy: 0.6502\n",
            "Epoch 7/20\n",
            "6752/6752 [==============================] - 505s 75ms/step - loss: 0.9922 - accuracy: 0.6536 - val_loss: 0.9888 - val_accuracy: 0.6522\n",
            "Epoch 8/20\n",
            "6752/6752 [==============================] - 505s 75ms/step - loss: 0.9855 - accuracy: 0.6559 - val_loss: 0.9901 - val_accuracy: 0.6538\n",
            "Epoch 9/20\n",
            "6752/6752 [==============================] - 518s 77ms/step - loss: 0.9805 - accuracy: 0.6570 - val_loss: 0.9867 - val_accuracy: 0.6532\n",
            "Epoch 10/20\n",
            "6752/6752 [==============================] - 522s 77ms/step - loss: 0.9761 - accuracy: 0.6587 - val_loss: 0.9797 - val_accuracy: 0.6571\n",
            "Epoch 11/20\n",
            "6752/6752 [==============================] - 522s 77ms/step - loss: 0.9725 - accuracy: 0.6601 - val_loss: 0.9760 - val_accuracy: 0.6573\n",
            "Epoch 12/20\n",
            "6752/6752 [==============================] - 507s 75ms/step - loss: 0.9691 - accuracy: 0.6616 - val_loss: 0.9744 - val_accuracy: 0.6580\n",
            "Epoch 13/20\n",
            "6752/6752 [==============================] - 509s 75ms/step - loss: 0.9663 - accuracy: 0.6626 - val_loss: 0.9818 - val_accuracy: 0.6554\n",
            "Epoch 14/20\n",
            "6752/6752 [==============================] - 521s 77ms/step - loss: 0.9639 - accuracy: 0.6642 - val_loss: 0.9789 - val_accuracy: 0.6584\n",
            "Epoch 15/20\n",
            "6752/6752 [==============================] - 520s 77ms/step - loss: 0.9618 - accuracy: 0.6650 - val_loss: 0.9664 - val_accuracy: 0.6621\n",
            "Epoch 16/20\n",
            "6752/6752 [==============================] - 505s 75ms/step - loss: 0.9599 - accuracy: 0.6657 - val_loss: 0.9658 - val_accuracy: 0.6614\n",
            "Epoch 17/20\n",
            "6752/6752 [==============================] - 503s 75ms/step - loss: 0.9584 - accuracy: 0.6659 - val_loss: 0.9651 - val_accuracy: 0.6636\n",
            "Epoch 18/20\n",
            "6752/6752 [==============================] - 512s 76ms/step - loss: 0.9572 - accuracy: 0.6674 - val_loss: 0.9644 - val_accuracy: 0.6646\n",
            "Epoch 19/20\n",
            "6752/6752 [==============================] - 510s 76ms/step - loss: 0.9554 - accuracy: 0.6680 - val_loss: 0.9651 - val_accuracy: 0.6619\n",
            "Epoch 20/20\n",
            "6752/6752 [==============================] - 498s 74ms/step - loss: 0.9537 - accuracy: 0.6685 - val_loss: 0.9608 - val_accuracy: 0.6642\n",
            "2251/2251 - 29s - loss: 0.9608 - accuracy: 0.6642 - 29s/epoch - 13ms/step\n",
            "Training time: 10215.865615844727s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  32\n",
            "Model loss:  0.9608111381530762\n",
            "Model accuracy:  0.6641626358032227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 8 - LSTM batch_size = 64"
      ],
      "metadata": {
        "id": "dF-KlSc0ufSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFR3HghwdhDg",
        "outputId": "4c2bd085-c6af-4b3b-89a4-7f53fbfe9723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "3376/3376 [==============================] - 284s 83ms/step - loss: 1.1320 - accuracy: 0.6103 - val_loss: 1.1058 - val_accuracy: 0.6097\n",
            "Epoch 2/20\n",
            "3376/3376 [==============================] - 282s 83ms/step - loss: 1.0956 - accuracy: 0.6195 - val_loss: 1.0827 - val_accuracy: 0.6234\n",
            "Epoch 3/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0712 - accuracy: 0.6274 - val_loss: 1.0684 - val_accuracy: 0.6322\n",
            "Epoch 4/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 1.0396 - accuracy: 0.6388 - val_loss: 1.0423 - val_accuracy: 0.6391\n",
            "Epoch 5/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 1.0224 - accuracy: 0.6443 - val_loss: 1.0156 - val_accuracy: 0.6433\n",
            "Epoch 6/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0120 - accuracy: 0.6471 - val_loss: 1.0089 - val_accuracy: 0.6472\n",
            "Epoch 7/20\n",
            "3376/3376 [==============================] - 280s 83ms/step - loss: 1.0041 - accuracy: 0.6494 - val_loss: 1.0134 - val_accuracy: 0.6442\n",
            "Epoch 8/20\n",
            "3376/3376 [==============================] - 279s 83ms/step - loss: 0.9976 - accuracy: 0.6516 - val_loss: 0.9967 - val_accuracy: 0.6477\n",
            "Epoch 9/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9918 - accuracy: 0.6532 - val_loss: 0.9913 - val_accuracy: 0.6520\n",
            "Epoch 10/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9862 - accuracy: 0.6551 - val_loss: 0.9957 - val_accuracy: 0.6521\n",
            "Epoch 11/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9818 - accuracy: 0.6569 - val_loss: 0.9993 - val_accuracy: 0.6539\n",
            "Epoch 12/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9774 - accuracy: 0.6578 - val_loss: 0.9821 - val_accuracy: 0.6551\n",
            "Epoch 13/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9738 - accuracy: 0.6587 - val_loss: 0.9769 - val_accuracy: 0.6575\n",
            "Epoch 14/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9699 - accuracy: 0.6597 - val_loss: 0.9761 - val_accuracy: 0.6562\n",
            "Epoch 15/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9666 - accuracy: 0.6609 - val_loss: 0.9761 - val_accuracy: 0.6579\n",
            "Epoch 16/20\n",
            "3376/3376 [==============================] - 278s 82ms/step - loss: 0.9631 - accuracy: 0.6628 - val_loss: 0.9685 - val_accuracy: 0.6598\n",
            "Epoch 17/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9602 - accuracy: 0.6635 - val_loss: 0.9797 - val_accuracy: 0.6552\n",
            "Epoch 18/20\n",
            "3376/3376 [==============================] - 277s 82ms/step - loss: 0.9577 - accuracy: 0.6642 - val_loss: 0.9669 - val_accuracy: 0.6625\n",
            "Epoch 19/20\n",
            "3376/3376 [==============================] - 281s 83ms/step - loss: 0.9553 - accuracy: 0.6657 - val_loss: 0.9633 - val_accuracy: 0.6633\n",
            "Epoch 20/20\n",
            "3376/3376 [==============================] - 279s 82ms/step - loss: 0.9532 - accuracy: 0.6668 - val_loss: 0.9606 - val_accuracy: 0.6638\n",
            "2251/2251 - 28s - loss: 0.9606 - accuracy: 0.6638 - 28s/epoch - 13ms/step\n",
            "Training time: 5575.135720014572s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  64\n",
            "Model loss:  0.9606304168701172\n",
            "Model accuracy:  0.6638432741165161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 9 - LSTM batch_size = 128"
      ],
      "metadata": {
        "id": "ArYTeK_9uh9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOUBLE CHECKED\n",
        "\n",
        "max_num_words = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "sentences = pad_sequences(sequences)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, test_size = 0.25, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(layers.Embedding(max_num_words, 50))\n",
        "LSTM_model.add(layers.LSTM(10))\n",
        "LSTM_model.add(layers.Dense(6,activation='softmax'))\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "start = time.time()\n",
        "LSTM_model.fit(X_train, y_train, epochs= epochs,batch_size = batch_size)\n",
        "stop = time.time()\n",
        "\n",
        "test_loss, test_acc = LSTM_model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Training time: {stop - start}s\")\n",
        "print(\"Maximum number of words in tokenizer: \", max_num_words)\n",
        "print(\"Number of Epochs for Training: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print('Model loss: ',test_loss)\n",
        "print('Model accuracy: ',test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXJRBF5ydhRR",
        "outputId": "ca881c4f-1bb9-4450-cfda-7f6303d443f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[22  6  3 ... 19 13  9]\n",
            " [ 0  0  0 ...  8 14  1]\n",
            " [ 0  0  0 ...  7 22  6]\n",
            " ...\n",
            " [ 0  0  0 ...  3  5 10]\n",
            " [ 0  0  0 ...  1  2 14]\n",
            " [ 5 14  1 ...  3  3 10]]\n",
            "216048 72017 216048 72017\n",
            "Epoch 1/20\n",
            "1688/1688 [==============================] - 172s 101ms/step - loss: 1.1396 - accuracy: 0.6122 - val_loss: 1.1016 - val_accuracy: 0.6228\n",
            "Epoch 2/20\n",
            "1688/1688 [==============================] - 162s 96ms/step - loss: 1.0920 - accuracy: 0.6248 - val_loss: 1.0835 - val_accuracy: 0.6274\n",
            "Epoch 3/20\n",
            "1688/1688 [==============================] - 170s 101ms/step - loss: 1.0741 - accuracy: 0.6291 - val_loss: 1.0671 - val_accuracy: 0.6311\n",
            "Epoch 4/20\n",
            "1688/1688 [==============================] - 162s 96ms/step - loss: 1.0559 - accuracy: 0.6345 - val_loss: 1.0509 - val_accuracy: 0.6356\n",
            "Epoch 5/20\n",
            "1688/1688 [==============================] - 162s 96ms/step - loss: 1.0403 - accuracy: 0.6388 - val_loss: 1.0317 - val_accuracy: 0.6401\n",
            "Epoch 6/20\n",
            "1688/1688 [==============================] - 163s 96ms/step - loss: 1.0285 - accuracy: 0.6426 - val_loss: 1.0307 - val_accuracy: 0.6365\n",
            "Epoch 7/20\n",
            "1688/1688 [==============================] - 170s 100ms/step - loss: 1.0186 - accuracy: 0.6451 - val_loss: 1.0119 - val_accuracy: 0.6451\n",
            "Epoch 8/20\n",
            "1688/1688 [==============================] - 170s 101ms/step - loss: 1.0128 - accuracy: 0.6470 - val_loss: 1.0078 - val_accuracy: 0.6467\n",
            "Epoch 9/20\n",
            "1688/1688 [==============================] - 172s 102ms/step - loss: 1.0075 - accuracy: 0.6486 - val_loss: 1.0088 - val_accuracy: 0.6467\n",
            "Epoch 10/20\n",
            "1688/1688 [==============================] - 170s 101ms/step - loss: 1.0033 - accuracy: 0.6500 - val_loss: 1.0013 - val_accuracy: 0.6497\n",
            "Epoch 11/20\n",
            "1688/1688 [==============================] - 162s 96ms/step - loss: 0.9985 - accuracy: 0.6514 - val_loss: 0.9999 - val_accuracy: 0.6498\n",
            "Epoch 12/20\n",
            "1688/1688 [==============================] - 161s 96ms/step - loss: 0.9942 - accuracy: 0.6528 - val_loss: 1.0016 - val_accuracy: 0.6461\n",
            "Epoch 13/20\n",
            "1688/1688 [==============================] - 162s 96ms/step - loss: 0.9911 - accuracy: 0.6537 - val_loss: 1.0006 - val_accuracy: 0.6481\n",
            "Epoch 14/20\n",
            "1688/1688 [==============================] - 171s 101ms/step - loss: 0.9879 - accuracy: 0.6551 - val_loss: 0.9952 - val_accuracy: 0.6521\n",
            "Epoch 15/20\n",
            "1688/1688 [==============================] - 163s 96ms/step - loss: 0.9851 - accuracy: 0.6558 - val_loss: 0.9862 - val_accuracy: 0.6542\n",
            "Epoch 16/20\n",
            "1688/1688 [==============================] - 171s 101ms/step - loss: 0.9821 - accuracy: 0.6567 - val_loss: 0.9975 - val_accuracy: 0.6495\n",
            "Epoch 17/20\n",
            "1688/1688 [==============================] - 167s 99ms/step - loss: 0.9801 - accuracy: 0.6571 - val_loss: 0.9809 - val_accuracy: 0.6553\n",
            "Epoch 18/20\n",
            "1688/1688 [==============================] - 174s 103ms/step - loss: 0.9772 - accuracy: 0.6578 - val_loss: 0.9855 - val_accuracy: 0.6564\n",
            "Epoch 19/20\n",
            "1688/1688 [==============================] - 173s 102ms/step - loss: 0.9746 - accuracy: 0.6589 - val_loss: 0.9867 - val_accuracy: 0.6574\n",
            "Epoch 20/20\n",
            "1688/1688 [==============================] - 173s 103ms/step - loss: 0.9730 - accuracy: 0.6593 - val_loss: 0.9759 - val_accuracy: 0.6563\n",
            "2251/2251 - 28s - loss: 0.9759 - accuracy: 0.6563 - 28s/epoch - 12ms/step\n",
            "Training time: 3384.1927909851074s\n",
            "Maximum number of words in tokenizer:  2000\n",
            "Number of Epochs for Training:  20\n",
            "Batch Size:  128\n",
            "Model loss:  0.9758654832839966\n",
            "Model accuracy:  0.656345009803772\n"
          ]
        }
      ]
    }
  ]
}
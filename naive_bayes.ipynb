{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring a Transformer-based approach on Sentiment Analysis for Application Review\n",
        "**Naïve Bayes**\n",
        "\n",
        "COMP 550 - Group Project\n",
        "\n",
        "Renchi Zhang | ID: 261110529"
      ],
      "metadata": {
        "id": "alTbQruMs1jB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u6G-qWqSqot",
        "outputId": "34eed980-f3b7-4770-9551-50bc20650c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"app_reviews\")\n",
        "\n",
        "import pandas as pd\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "data = df[[\"review\", \"star\"]]\n",
        "# data = data.loc[:100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa6Zj5L-hzOW",
        "outputId": "e74c2723-116c-4de8-b47a-d57f662ac740"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "def format_time(seconds):\n",
        "    return str(timedelta(seconds=int(seconds)))"
      ],
      "metadata": {
        "id": "UNuYE-iEvs1h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import time\n",
        "\n",
        "def review_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    lemmatize = WordNetLemmatizer()\n",
        "    text = [lemmatize.lemmatize(word) for word in text]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    return text\n",
        "\n",
        "preprocess_start_time = time.time()\n",
        "data['review'] = data['review'].apply(lambda x: review_preprocess(x))\n",
        "data['review'] = data['review'].astype(str)\n",
        "preprocess_time = time.time()-preprocess_start_time\n",
        "print(f\"Time for preprocessing: {preprocess_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj2kySGKUDyc",
        "outputId": "70e1b252-07ad-42d8-f7fd-3d0dbc40afb8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-cd96224fc024>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['review'] = data['review'].apply(lambda x: review_preprocess(x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for preprocessing: 134.30165910720825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-cd96224fc024>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['review'] = data['review'].astype(str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "X, y = data['review'], data['star']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)  # 25% for testing"
      ],
      "metadata": {
        "id": "PuNMeQ6eiLXe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid search on naïve bayes\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "review_cls_mnb = Pipeline([('vectorizer', CountVectorizer()),\n",
        "                           ('feature extractor', TfidfTransformer()),\n",
        "                           ('cls-mnb', MultinomialNB(force_alpha=True))\n",
        "                           ])\n",
        "\n",
        "parameters_mnb = {'vectorizer__ngram_range': [(1, 1), (2, 2), (1, 2), (3, 3), (1, 3), (2, 3)],\n",
        "                  'feature extractor__use_idf': (True, False),\n",
        "                  'feature extractor__smooth_idf': (True, False),\n",
        "                  # 'cls-mnb__alpha': np.geomspace(1e-10, 1e10, num=21, endpoint=True)\n",
        "                  'cls-mnb__alpha': np.geomspace(1e-2, 1e0, num=10, endpoint=True)\n",
        "                  }\n",
        "\n",
        "grid_start_time = time.time()\n",
        "gs_cls_mnb = GridSearchCV(review_cls_mnb, parameters_mnb, scoring='accuracy')\n",
        "gs_cls_mnb = gs_cls_mnb.fit(X_train, y_train)\n",
        "grid_search_training_time = time.time() - grid_start_time\n",
        "print(f\"Time for grid search and training: {grid_search_training_time}\")\n",
        "\n",
        "print('The best accuracy for training set is: ' + str(gs_cls_mnb.best_score_))\n",
        "cls_mnb_best_params = gs_cls_mnb.best_params_\n",
        "\n",
        "print(\"\\nHyper-parameters of Naive Bayes classifier:\")\n",
        "vectorizer = CountVectorizer(\n",
        "    ngram_range=cls_mnb_best_params['vectorizer__ngram_range']\n",
        ")\n",
        "tfidf_transformer = TfidfTransformer(\n",
        "    use_idf=cls_mnb_best_params['feature extractor__use_idf'],\n",
        "    smooth_idf=cls_mnb_best_params['feature extractor__smooth_idf']\n",
        ")\n",
        "classifier = MultinomialNB(\n",
        "    force_alpha=True,\n",
        "    alpha=cls_mnb_best_params['cls-mnb__alpha']\n",
        ")\n",
        "\n",
        "print(\"ngram_range of vectorizer: \", str(cls_mnb_best_params['vectorizer__ngram_range']))\n",
        "print(\"use_idf of tfidf_transformer: \", str(cls_mnb_best_params['feature extractor__use_idf']))\n",
        "print(\"smooth_idf of tfidf_transformer: \", str(cls_mnb_best_params['feature extractor__smooth_idf']))\n",
        "print(\"alpha of MultinomialNB: \", str(cls_mnb_best_params['cls-mnb__alpha']))\n"
      ],
      "metadata": {
        "id": "F1QpteVlkNZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd28f3a4-2a07-41ba-d594-0d4cb835c555"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for grid search and training: 9935.305357694626\n",
            "The best accuracy for training set is: 0.6859679334828763\n",
            "\n",
            "Hyper-parameters of Naive Bayes classifier:\n",
            "ngram_range of vectorizer:  (1, 2)\n",
            "use_idf of tfidf_transformer:  False\n",
            "smooth_idf of tfidf_transformer:  True\n",
            "alpha of MultinomialNB:  0.046415888336127774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_start_time = time.time()\n",
        "X_train_transformed = vectorizer.fit_transform(X_train)\n",
        "X_train_transformed = tfidf_transformer.fit_transform(X_train_transformed)\n",
        "classifier.fit(X_train_transformed, y_train)\n",
        "fit_end_time = time.time()\n",
        "print(f\"Time for training and fitting: {fit_end_time-fit_start_time}\")\n",
        "\n",
        "predict_start_time = time.time()\n",
        "X_test_transformed = vectorizer.transform(X_test)\n",
        "X_test_transformed = tfidf_transformer.transform(X_test_transformed)\n",
        "y_test_pred = classifier.predict(X_test_transformed)\n",
        "predict_end_time = time.time()\n",
        "print(f\"Time for prediction: {predict_end_time-predict_start_time}\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('The accuracy for the testing set is: ' + str(accuracy))\n",
        "\n",
        "\n",
        "log_probabilities = classifier.predict_log_proba(X_test_transformed)\n",
        "cross_entropy_loss = log_loss(y_test, log_probabilities)\n",
        "print(\"Cross Entropy Loss:\", cross_entropy_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpPUJaX-GQoK",
        "outputId": "703263ab-8f9d-4bc2-edbb-02e6924d8129"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for training and fitting: 7.994792938232422\n",
            "Time for prediction: 1.5407071113586426\n",
            "The accuracy for the testing set is: 0.6870738853326298\n",
            "Cross Entropy Loss: 1.6094379124340998\n"
          ]
        }
      ]
    }
  ]
}
